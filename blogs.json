{"status":"ok","feed":{"url":"https://medium.com/feed/@raviteja0096","title":"Stories by Raviteja Tholupunoori on Medium","link":"https://medium.com/@raviteja0096?source=rss-3ead5bb356d4------2","author":"","description":"Stories by Raviteja Tholupunoori on Medium","image":"https://cdn-images-1.medium.com/fit/c/150/150/0*KO1LKfm7xvYxO-9b."},"items":[{"title":"Apache Airflow Architecture Simplified","pubDate":"2024-07-18 12:29:27","link":"https://medium.com/apache-airflow/airflow-architecture-simplified-3d582fc3ccb0?source=rss-3ead5bb356d4------2","guid":"https://medium.com/p/3d582fc3ccb0","author":"Raviteja Tholupunoori","thumbnail":"","description":"\n<h3>Apache Airflow Architecture</h3>\n<p><a href=\"https://airflow.apache.org/\">Apache Airflow</a> is an open-source platform designed to orchestrate complex data workflows. It uses Directed Acyclic Graphs (DAGs) to define a series of tasks and their dependencies. Airflow is made up of several microservices that collaborate to execute these tasks. Here\u2019s a straightforward breakdown of the key components of airflow Architecture.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*SxYErMAzuN0MXQryQfpGLQ.gif\"></figure><h3><strong>Components:</strong></h3>\n<p>\ud835\uddea\ud835\uddf2\ud835\uddef \ud835\udde6\ud835\uddf2\ud835\uddff\ud835\ude03\ud835\uddf2\ud835\uddff \ud83c\udf10: Airflow UI where you can monitor, and manage DAGs, Variables, connections and check the logs. It provides a dashboard that helps you visualize your data workflows, check their progress, and troubleshoot any\u00a0issues.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/810/1*xJvNvs6AaCus10W38OF5Uw.gif\"></figure><p>\ud835\udde6\ud835\uddf0\ud835\uddf5\ud835\uddf2\ud835\uddf1\ud835\ude02\ud835\uddf9\ud835\uddf2\ud835\uddff \ud83d\udd70\ufe0f: It is responsible for managing the execution of tasks. It monitors the DAGs and schedules tasks based on their dependencies and timing configurations. It makes sure that tasks are executed in the right order and at the right\u00a0time.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*FdXjpygIU1EdBaZ7n8vpxg.gif\"></figure><p>\ud835\uddd8\ud835\ude05\ud835\uddf2\ud835\uddf0\ud835\ude02\ud835\ude01\ud835\uddfc\ud835\uddff \u2699\ufe0f: The Executor\u2019s primary role involves executing tasks actively. It interacts with the Scheduler to obtain task details and initiates the required processes or containers for task execution.</p>\n<p>Airflow offers various Executor types like LocalExecutor, CeleryExecutor, and KubernetesExecutor, each tailored to specific infrastructure setups and operational needs.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1016/1*vVCX0toozRV6kwlc9SWVqg.gif\"></figure><p>\ud835\uddea\ud835\uddfc\ud835\uddff\ud835\uddf8\ud835\uddf2\ud835\uddff \ud83d\udc77: The Worker is a component that performs the tasks assigned by the Executor. Depending on the chosen Executor, it can be a separate process or container. Workers are responsible for executing the actual code or scripts defined in your tasks and reporting their status back to the Executor.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/948/1*ylFHMKEf8yvk35XcJWiftA.gif\"></figure><p>\ud835\udde0\ud835\uddf2\ud835\ude01\ud835\uddee\ud835\uddf1\ud835\uddee\ud835\ude01\ud835\uddee \ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee\ud835\uddef\ud835\uddee\ud835\ude00\ud835\uddf2 \ud83d\udee2: This is where Airflow keeps track of all your workflows, including details about the tasks you\u2019ve set up and how they\u2019ve run in the past. It\u2019s like a central hub for storing and organizing everything related to your scheduled tasks. This helps you keep an eye on how things are progressing and troubleshoot any issues that might come up. Airflow gives you the flexibility to use different databases like PostgreSQL, MySQL, or SQLite to store this information, depending on what works best for your\u00a0setup.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1012/1*dQeTkFdzs8P_WCOAt68r7Q.gif\"></figure><p>\ud835\udde0\ud835\uddf2\ud835\ude00\ud835\ude00\ud835\uddee\ud835\uddf4\ud835\uddf2 \ud835\uddd5\ud835\uddff\ud835\uddfc\ud835\uddf8\ud835\uddf2\ud835\uddff \u2709\ufe0e \u2709\ufe0e \u2709\ufe0e(\ud835\uddfc\ud835\uddfd\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb\ud835\uddee\ud835\uddf9): In setups where the CeleryExecutor is used for distributing tasks, a message broker plays a crucial role. This broker, like RabbitMQ or Redis, acts as a middleman between the Scheduler and the Workers. It ensures smooth communication by passing task details from the Scheduler to the Workers, ensuring tasks are executed reliably and efficiently across the distributed system.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/876/1*eOsKdwCTzqwGHs_YaDIn4A.gif\"></figure><p>If you like the images you can all the designs like GIFs, Images &amp; drawio templates in my repository mentioned below.</p>\n<p>Repo: <a href=\"https://github.com/raviteja10096/Airflow/tree/main/Airflow_Architecture\"><em>https://github.com/raviteja-git/Airflow/tree/main/Airflow%20Architecture</em></a></p>\n<p>Feel free to reach out to me on <a href=\"https://www.linkedin.com/in/raviteja0096/\">LinkedIn</a> if you have any questions or want to learn more about the airflow!\u00a0\ud83d\ude0a</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=3d582fc3ccb0\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/apache-airflow/airflow-architecture-simplified-3d582fc3ccb0\">Apache Airflow Architecture Simplified</a> was originally published in <a href=\"https://medium.com/apache-airflow\">Apache Airflow</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<h3>Apache Airflow Architecture</h3>\n<p><a href=\"https://airflow.apache.org/\">Apache Airflow</a> is an open-source platform designed to orchestrate complex data workflows. It uses Directed Acyclic Graphs (DAGs) to define a series of tasks and their dependencies. Airflow is made up of several microservices that collaborate to execute these tasks. Here\u2019s a straightforward breakdown of the key components of airflow Architecture.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*SxYErMAzuN0MXQryQfpGLQ.gif\"></figure><h3><strong>Components:</strong></h3>\n<p>\ud835\uddea\ud835\uddf2\ud835\uddef \ud835\udde6\ud835\uddf2\ud835\uddff\ud835\ude03\ud835\uddf2\ud835\uddff \ud83c\udf10: Airflow UI where you can monitor, and manage DAGs, Variables, connections and check the logs. It provides a dashboard that helps you visualize your data workflows, check their progress, and troubleshoot any\u00a0issues.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/810/1*xJvNvs6AaCus10W38OF5Uw.gif\"></figure><p>\ud835\udde6\ud835\uddf0\ud835\uddf5\ud835\uddf2\ud835\uddf1\ud835\ude02\ud835\uddf9\ud835\uddf2\ud835\uddff \ud83d\udd70\ufe0f: It is responsible for managing the execution of tasks. It monitors the DAGs and schedules tasks based on their dependencies and timing configurations. It makes sure that tasks are executed in the right order and at the right\u00a0time.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*FdXjpygIU1EdBaZ7n8vpxg.gif\"></figure><p>\ud835\uddd8\ud835\ude05\ud835\uddf2\ud835\uddf0\ud835\ude02\ud835\ude01\ud835\uddfc\ud835\uddff \u2699\ufe0f: The Executor\u2019s primary role involves executing tasks actively. It interacts with the Scheduler to obtain task details and initiates the required processes or containers for task execution.</p>\n<p>Airflow offers various Executor types like LocalExecutor, CeleryExecutor, and KubernetesExecutor, each tailored to specific infrastructure setups and operational needs.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1016/1*vVCX0toozRV6kwlc9SWVqg.gif\"></figure><p>\ud835\uddea\ud835\uddfc\ud835\uddff\ud835\uddf8\ud835\uddf2\ud835\uddff \ud83d\udc77: The Worker is a component that performs the tasks assigned by the Executor. Depending on the chosen Executor, it can be a separate process or container. Workers are responsible for executing the actual code or scripts defined in your tasks and reporting their status back to the Executor.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/948/1*ylFHMKEf8yvk35XcJWiftA.gif\"></figure><p>\ud835\udde0\ud835\uddf2\ud835\ude01\ud835\uddee\ud835\uddf1\ud835\uddee\ud835\ude01\ud835\uddee \ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee\ud835\uddef\ud835\uddee\ud835\ude00\ud835\uddf2 \ud83d\udee2: This is where Airflow keeps track of all your workflows, including details about the tasks you\u2019ve set up and how they\u2019ve run in the past. It\u2019s like a central hub for storing and organizing everything related to your scheduled tasks. This helps you keep an eye on how things are progressing and troubleshoot any issues that might come up. Airflow gives you the flexibility to use different databases like PostgreSQL, MySQL, or SQLite to store this information, depending on what works best for your\u00a0setup.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1012/1*dQeTkFdzs8P_WCOAt68r7Q.gif\"></figure><p>\ud835\udde0\ud835\uddf2\ud835\ude00\ud835\ude00\ud835\uddee\ud835\uddf4\ud835\uddf2 \ud835\uddd5\ud835\uddff\ud835\uddfc\ud835\uddf8\ud835\uddf2\ud835\uddff \u2709\ufe0e \u2709\ufe0e \u2709\ufe0e(\ud835\uddfc\ud835\uddfd\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb\ud835\uddee\ud835\uddf9): In setups where the CeleryExecutor is used for distributing tasks, a message broker plays a crucial role. This broker, like RabbitMQ or Redis, acts as a middleman between the Scheduler and the Workers. It ensures smooth communication by passing task details from the Scheduler to the Workers, ensuring tasks are executed reliably and efficiently across the distributed system.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/876/1*eOsKdwCTzqwGHs_YaDIn4A.gif\"></figure><p>If you like the images you can all the designs like GIFs, Images &amp; drawio templates in my repository mentioned below.</p>\n<p>Repo: <a href=\"https://github.com/raviteja10096/Airflow/tree/main/Airflow_Architecture\"><em>https://github.com/raviteja-git/Airflow/tree/main/Airflow%20Architecture</em></a></p>\n<p>Feel free to reach out to me on <a href=\"https://www.linkedin.com/in/raviteja0096/\">LinkedIn</a> if you have any questions or want to learn more about the airflow!\u00a0\ud83d\ude0a</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=3d582fc3ccb0\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/apache-airflow/airflow-architecture-simplified-3d582fc3ccb0\">Apache Airflow Architecture Simplified</a> was originally published in <a href=\"https://medium.com/apache-airflow\">Apache Airflow</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["engineering","airflow","architecture","apache-airflow","data-engineering"]},{"title":"Handbook for Becoming an Astronomer Champion for Apache Airflow","pubDate":"2024-07-15 11:17:09","link":"https://medium.com/apache-airflow/handbook-for-becoming-an-apache-airflow-champion-de5b76b94acd?source=rss-3ead5bb356d4------2","guid":"https://medium.com/p/de5b76b94acd","author":"Raviteja Tholupunoori","thumbnail":"","description":"\n<h4>My Apache Airflow\u00a0Journey</h4>\n<p>Back in 2019, I started working on a project to build a customer data platform (CDP) for a client. This involved processing massive amounts of data through various microservices. Initially, we used <a href=\"https://www.talend.com/\">Talend</a>, an ETL tool, but managing it became cumbersome. Even minor changes to the pipeline or data schema required rebuilding and redeploying the entire Talend\u00a0job.</p>\n<p>This led me to explore alternatives, and that\u2019s when I came across <a href=\"https://airflow.apache.org/\">Apache Airflow</a>. I dug into how it worked and became convinced that it was a better solution for our needs. I presented my findings to my team, and they agreed. That\u2019s how my journey with Airflow\u00a0began!</p>\n<p>Using Airflow I have built Cloud Agnostic Solutions and deployed it in Azure Kubernetes Service and Google kubernetes Engine(Both in Standard and Auto Pilot Clusters)and currently working on AWS\u00a0. While doing i got very good exposure to Airflow and also to its Infrastructure needed for different components.</p>\n<h4>Airflow Fundamentals Certification</h4>\n<p>With all the knowledge that i have gained over 4\u20135 years i was able to directly crack Airflow Fundamentals Certification</p>\n<p>Learning Path: <a href=\"https://academy.astronomer.io/path/airflow-101\">https://academy.astronomer.io/path/airflow-101</a></p>\n<p>Certificate: <a href=\"https://www.credly.com/badges/cd274021-dae6-4d88-a954-142270d6b231/public_url\">https://www.credly.com/badges/cd274021-dae6-4d88-a954-142270d6b231/public_url</a></p>\n<h4>My Contribution to Airflow Community</h4>\n<p>I have started helping different Teams within my organisation and even in other open source platforms with troubleshooting airflow issues and proposing workflow enhancements, fostering a collaborative and efficient environment. This helped to gain more Knowledge and solve different usecases.</p>\n<h4>Airflow Champion\u2019s Journey</h4>\n<p>I came accross <a href=\"https://www.astronomer.io/champions/\">Astronomer\u2019s Champions for Apache Airflow </a>while exploring my certificate in Credly and i have applied to the program. All the work (above mentioned) helped me to apply directly without any extra\u00a0efforts.</p>\n<p>Post applying my application was reviewed by Astronomer\u2019s team and i was selected as Champion.</p>\n<p><strong>Confirmation Mail</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*1-DW4zGmt6JdgBq1qRuKUA.png\"></figure><p><strong>Certificate and\u00a0Badge</strong></p>\n<p>Certificate\u00a0: <a href=\"https://www.credly.com/badges/f31d1c0d-cb76-44a0-82fa-c24cfbd8e5aa/public_url\">https://www.credly.com/badges/f31d1c0d-cb76-44a0-82fa-c24cfbd8e5aa/public_url</a></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/390/1*wyeEepKsVsGkgBpXwK25Yg.png\"></figure><p><strong>Linked in Post by Astronomer</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/976/1*X_yiC3wC3fqtKFDpN5q6iA.png\"></figure><p><a href=\"https://www.linkedin.com/posts/astronomer_introducing-cohort-3-of-the-astronomer-champions-activity-7211820957626195968-meNk?utm_source=share&amp;utm_medium=member_desktop\">Astronomer on LinkedIn: Introducing Cohort 3 of the Astronomer Champions Program for Apache...</a></p>\n<p><strong>Astronomer Champions Announcement</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*1rNlre7wcgaaTQPFoRmbXw.png\"></figure><p><a href=\"https://www.astronomer.io/blog/introducing-cohort-3-astronomer-champions-program-apache-airflow/\">Introducing Cohort 3 of the Astronomer Champions Program for Apache Airflow\u00ae!</a></p>\n<p>I was added to official Slack Channel of Airflow, was included in Airflow townhalls and Recieved Airflow SWAG today. Going forward i\u2019ll utilise other benefits provided though this\u00a0program.</p>\n<h4>How to Apply to Astronomer Airflow Champions:</h4>\n<p>Champions are community leaders who dedicate their time and expertise to enhancing the understanding and awareness of the Airflow product and mission. Below is the application Link and Pre-requisites for applying for the\u00a0Program.</p>\n<p><strong>About the Program\u200a\u2014\u200a</strong>It\u2019s an initiative dedicated to fostering passionate advocates of Apache Airflow. This program is designed to empower and elevate the knowledge, influence, and leadership of its committed members who champion the cause of Apache Airflow\u00ae technology.</p>\n<p><strong>Astronomer Champion\u2019s Program</strong>\u200a\u2014\u200a<a href=\"https://www.astronomer.io/champions/\">https://www.astronomer.io/champions/</a></p>\n<p><strong>Application Link</strong>\u200a\u2014\u200a<a href=\"https://docs.google.com/forms/d/e/1FAIpQLScSKVzfRf3wppjUbzx0dUzFLwoP66ZfQ6rYLjk9ZASzpKA2Dw/viewform\">Application Form</a></p>\n<h4>Pre requisites</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*JnliKnt_jRLo3JOEgC1hIw.png\"></figure><p>Once your application is reviewed and selected for a cohort you\u2019ll recieve a mail and you can start your chamipion journey.</p>\n<p><strong>What it means to be\u00a0Champion</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*S7LBKeUVEsl8V9kQ-Iushw.png\"></figure><p><strong>Benefits:</strong></p>\n<p>As a member, you\u2019ll enjoy a variety of exclusive benefits, including complimentary certifications, access to Airflow contributors, beta access to new features, academy courses, and certifications\u2026 and of course,\u00a0swag</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*1XFkyszjrog_8wkM-5E8qw.png\"></figure><p>Feel free to reach out to me on <a href=\"https://www.linkedin.com/in/raviteja0096/\">LinkedIn</a> if you have any questions or want to learn more about the program!\u00a0\ud83d\ude0a</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=de5b76b94acd\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/apache-airflow/handbook-for-becoming-an-apache-airflow-champion-de5b76b94acd\">Handbook for Becoming an Astronomer Champion for Apache Airflow \ud83c\udfc6</a> was originally published in <a href=\"https://medium.com/apache-airflow\">Apache Airflow</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<h4>My Apache Airflow\u00a0Journey</h4>\n<p>Back in 2019, I started working on a project to build a customer data platform (CDP) for a client. This involved processing massive amounts of data through various microservices. Initially, we used <a href=\"https://www.talend.com/\">Talend</a>, an ETL tool, but managing it became cumbersome. Even minor changes to the pipeline or data schema required rebuilding and redeploying the entire Talend\u00a0job.</p>\n<p>This led me to explore alternatives, and that\u2019s when I came across <a href=\"https://airflow.apache.org/\">Apache Airflow</a>. I dug into how it worked and became convinced that it was a better solution for our needs. I presented my findings to my team, and they agreed. That\u2019s how my journey with Airflow\u00a0began!</p>\n<p>Using Airflow I have built Cloud Agnostic Solutions and deployed it in Azure Kubernetes Service and Google kubernetes Engine(Both in Standard and Auto Pilot Clusters)and currently working on AWS\u00a0. While doing i got very good exposure to Airflow and also to its Infrastructure needed for different components.</p>\n<h4>Airflow Fundamentals Certification</h4>\n<p>With all the knowledge that i have gained over 4\u20135 years i was able to directly crack Airflow Fundamentals Certification</p>\n<p>Learning Path: <a href=\"https://academy.astronomer.io/path/airflow-101\">https://academy.astronomer.io/path/airflow-101</a></p>\n<p>Certificate: <a href=\"https://www.credly.com/badges/cd274021-dae6-4d88-a954-142270d6b231/public_url\">https://www.credly.com/badges/cd274021-dae6-4d88-a954-142270d6b231/public_url</a></p>\n<h4>My Contribution to Airflow Community</h4>\n<p>I have started helping different Teams within my organisation and even in other open source platforms with troubleshooting airflow issues and proposing workflow enhancements, fostering a collaborative and efficient environment. This helped to gain more Knowledge and solve different usecases.</p>\n<h4>Airflow Champion\u2019s Journey</h4>\n<p>I came accross <a href=\"https://www.astronomer.io/champions/\">Astronomer\u2019s Champions for Apache Airflow </a>while exploring my certificate in Credly and i have applied to the program. All the work (above mentioned) helped me to apply directly without any extra\u00a0efforts.</p>\n<p>Post applying my application was reviewed by Astronomer\u2019s team and i was selected as Champion.</p>\n<p><strong>Confirmation Mail</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*1-DW4zGmt6JdgBq1qRuKUA.png\"></figure><p><strong>Certificate and\u00a0Badge</strong></p>\n<p>Certificate\u00a0: <a href=\"https://www.credly.com/badges/f31d1c0d-cb76-44a0-82fa-c24cfbd8e5aa/public_url\">https://www.credly.com/badges/f31d1c0d-cb76-44a0-82fa-c24cfbd8e5aa/public_url</a></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/390/1*wyeEepKsVsGkgBpXwK25Yg.png\"></figure><p><strong>Linked in Post by Astronomer</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/976/1*X_yiC3wC3fqtKFDpN5q6iA.png\"></figure><p><a href=\"https://www.linkedin.com/posts/astronomer_introducing-cohort-3-of-the-astronomer-champions-activity-7211820957626195968-meNk?utm_source=share&amp;utm_medium=member_desktop\">Astronomer on LinkedIn: Introducing Cohort 3 of the Astronomer Champions Program for Apache...</a></p>\n<p><strong>Astronomer Champions Announcement</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*1rNlre7wcgaaTQPFoRmbXw.png\"></figure><p><a href=\"https://www.astronomer.io/blog/introducing-cohort-3-astronomer-champions-program-apache-airflow/\">Introducing Cohort 3 of the Astronomer Champions Program for Apache Airflow\u00ae!</a></p>\n<p>I was added to official Slack Channel of Airflow, was included in Airflow townhalls and Recieved Airflow SWAG today. Going forward i\u2019ll utilise other benefits provided though this\u00a0program.</p>\n<h4>How to Apply to Astronomer Airflow Champions:</h4>\n<p>Champions are community leaders who dedicate their time and expertise to enhancing the understanding and awareness of the Airflow product and mission. Below is the application Link and Pre-requisites for applying for the\u00a0Program.</p>\n<p><strong>About the Program\u200a\u2014\u200a</strong>It\u2019s an initiative dedicated to fostering passionate advocates of Apache Airflow. This program is designed to empower and elevate the knowledge, influence, and leadership of its committed members who champion the cause of Apache Airflow\u00ae technology.</p>\n<p><strong>Astronomer Champion\u2019s Program</strong>\u200a\u2014\u200a<a href=\"https://www.astronomer.io/champions/\">https://www.astronomer.io/champions/</a></p>\n<p><strong>Application Link</strong>\u200a\u2014\u200a<a href=\"https://docs.google.com/forms/d/e/1FAIpQLScSKVzfRf3wppjUbzx0dUzFLwoP66ZfQ6rYLjk9ZASzpKA2Dw/viewform\">Application Form</a></p>\n<h4>Pre requisites</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*JnliKnt_jRLo3JOEgC1hIw.png\"></figure><p>Once your application is reviewed and selected for a cohort you\u2019ll recieve a mail and you can start your chamipion journey.</p>\n<p><strong>What it means to be\u00a0Champion</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*S7LBKeUVEsl8V9kQ-Iushw.png\"></figure><p><strong>Benefits:</strong></p>\n<p>As a member, you\u2019ll enjoy a variety of exclusive benefits, including complimentary certifications, access to Airflow contributors, beta access to new features, academy courses, and certifications\u2026 and of course,\u00a0swag</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*1XFkyszjrog_8wkM-5E8qw.png\"></figure><p>Feel free to reach out to me on <a href=\"https://www.linkedin.com/in/raviteja0096/\">LinkedIn</a> if you have any questions or want to learn more about the program!\u00a0\ud83d\ude0a</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=de5b76b94acd\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/apache-airflow/handbook-for-becoming-an-apache-airflow-champion-de5b76b94acd\">Handbook for Becoming an Astronomer Champion for Apache Airflow \ud83c\udfc6</a> was originally published in <a href=\"https://medium.com/apache-airflow\">Apache Airflow</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["astronomer","data-engineering","data-pipeline","apache-airflow","airflow"]},{"title":"Solving Data Pipeline Challenges with Apache Airflow: A Real-Life Example","pubDate":"2024-07-09 15:53:35","link":"https://medium.com/apache-airflow/solving-data-pipeline-challenges-with-apache-airflow-a-real-life-example-2049e555f9c4?source=rss-3ead5bb356d4------2","guid":"https://medium.com/p/2049e555f9c4","author":"Raviteja Tholupunoori","thumbnail":"","description":"\n<p>Imagine\ud83d\udcac you are a data engineer at a growing tech company, and one of your key responsibilities is to ensure that data from various sources is collected, transformed, and loaded into a central data warehouse for analysis. The complexity of managing these data pipelines increases as the volume of data grows, leading to frequent errors, missed deadlines, and a lot of manual intervention.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*EDZ7cf3ZQNUV_rOQun9HNA.jpeg\"></figure><h3>My Journey with\u00a0Airflow</h3>\n<p>Back in 2019, I started working on a project to build a customer data platform (CDP) for a client. This involved processing massive amounts of data through various microservices. Initially, we used Talend, an ETL tool, but managing it became cumbersome. Even minor changes to the pipeline or data schema required rebuilding and redeploying the entire Talend\u00a0job.</p>\n<p>This led me to explore alternatives, and that\u2019s when I came across Apache Airflow. I dug into how it worked and became convinced that it was a better solution for our needs. I presented my findings to my team, and they agreed. That\u2019s how my journey with Airflow began! Post this I worked almost for 3\u20134 years and gained much knowledged and even solved different use cases using Apache\u00a0Airflow</p>\n<p><strong>High Level Architecture Diagram</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/614/1*sTvGUcH4jjI-2rHC-vFSWA.png\"></figure><h3>About Airflow:</h3>\n<p>Apache Airflow is an open-source platform to programmatically author, schedule, and monitor workflows. Since its inception at Airbnb in 2014, Airflow has grown to become a widely adopted tool for orchestrating complex computational workflows and data processing pipelines. With its robust and extensible framework, Airflow is a top choice for data engineers and developers aiming to automate and manage their workflows.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*N2YxUJvf5046PZPv.png\"></figure><h3>Use cases</h3>\n<p>Apart form the above mentioned usecase we can solve different usecase as\u00a0below:</p>\n<ol>\n<li>\n<strong>ETL Pipelines</strong>: Airflow handles ETL(Extract, Transform, Load) tasks with grace. It schedules and monitors these pipelines, ensuring your data remains consistent and reliable, just like a diligent librarian cataloging new\u00a0books.</li>\n<li>\n<strong>Data Warehousing</strong>: Airflow helps streamline the process of loading data into warehouses, performing necessary transformations along the way making use of different operators in\u00a0built.</li>\n<li>\n<strong>Machine Learning Pipelines</strong>: Airflow can automates multiple steps like data preprocessing, model training, evaluation, and deployment ensuring your machine learning workflows run smoothly and efficiently, like a well-rehearsed symphony.</li>\n<li>\n<strong>Data Processing</strong>: Airflow coordinates tasks such as data cleaning, validation, and aggregation, acting as a reliable assistant that keeps everything on\u00a0track.</li>\n<li>\n<strong>DevOps Automation</strong>: Airflow can automate tasks like running backups, monitoring system health, and deploying applications, making it an invaluable tool for maintaining operational efficiency.</li>\n</ol>\n<h3>\n<strong>Airflow Pros </strong>\ud83d\udc4d\u00a0<strong>:</strong>\n</h3>\n<ul>\n<li>\n<strong>Scalability </strong>\ud83d\udcc8\ud83d\udd1d\ud83d\udcca: Airflow scales seamlessly, handling very large workflows and distributing tasks across multiple\u00a0workers.</li>\n<li>\n<strong>Flexibility </strong>\ud83e\udd38\u200d\u2642\ufe0f\ud83d\udd04\ud83c\udf00: With Python-based DAG definitions, You can easily customize your workflows to fit your unique requirements</li>\n<li>\n<strong>Extensibility </strong>\ud83e\udde9\ud83d\ude80\ud83d\udd27: Airflow\u2019s built-in operators(Bash Operators, Python Operators, Kubernetes Pod Operators and others) and the ability to create custom operators allow it to integrate with numerous external systems and\u00a0APIs.</li>\n<li>\n<strong>Community and Ecosystem </strong>\ud83d\udc6b\ud83c\udf0e: Being an Open source tool, Airflow benefits from a large and active community. This ensures continuous improvements and a wealth of plugins and integrations. It\u2019s like being part of a global network of experts, always ready to lend a helping\u00a0hand.</li>\n<li>\n<strong>UI and Monitoring </strong>\ud83d\udcf1\ud83d\udc41\ufe0f\u200d\ud83d\udde8\ufe0f\ud83d\udcca: The web interface provides a clear view of workflow status, logs, and task details, making debugging and monitoring a\u00a0breeze.</li>\n</ul>\n<h3>\n<strong>Airflow Cons\u00a0</strong>\ud83d\udc4e<strong>:</strong>\n</h3>\n<ul>\n<li>\n<strong>Complexity </strong>\ud83c\udf00\ud83d\udd0d\ud83d\udd27: The learning curve can be steep, especially for those new to Python or workflow orchestration. It\u2019s like learning to play a complex musical instrument\u200a\u2014\u200achallenging but rewarding once mastered.</li>\n<li>\n<strong>Resource Intensive </strong>\u2699\ufe0f\ud83d\udd0b\ud83d\udca1: Airflow can consume significant system resources, particularly for large DAGs and high-frequency task execution. Managing these resources effectively is crucial to maintain optimal performance.</li>\n<li>\n<strong>Operational Overhead </strong>\u23f3\ud83d\udd04: Maintaining an Airflow deployment, including managing the database and worker nodes, can be complex and require significant operational effort. It\u2019s like running a large-scale operation where every detail\u00a0matters.</li>\n<li>\n<strong>Dependency Management </strong>\ud83e\udde9\ud83d\udd17\ud83d\udce6: Handling Python dependencies across different tasks can be challenging, often requiring careful management of virtual environments or containers.</li>\n</ul>\n<h3>Conclusion:</h3>\n<p>Its has been almost 4\u20135 years i have been working on airflow staring from writing simple tasks and now able to deploy it in different cloud platforms liks GCP, Azure and AWS was an amazing Journey. Once you master this tool, it will be a game-changer for your data workflows.</p>\n<p>Airflow is definitely worth exploring. My experience has been overwhelmingly positive, and it\u2019s become a critical tool for our data engineering and workflow orchestration.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=2049e555f9c4\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/apache-airflow/solving-data-pipeline-challenges-with-apache-airflow-a-real-life-example-2049e555f9c4\">Solving Data Pipeline Challenges with Apache Airflow: A Real-Life Example</a> was originally published in <a href=\"https://medium.com/apache-airflow\">Apache Airflow</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<p>Imagine\ud83d\udcac you are a data engineer at a growing tech company, and one of your key responsibilities is to ensure that data from various sources is collected, transformed, and loaded into a central data warehouse for analysis. The complexity of managing these data pipelines increases as the volume of data grows, leading to frequent errors, missed deadlines, and a lot of manual intervention.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*EDZ7cf3ZQNUV_rOQun9HNA.jpeg\"></figure><h3>My Journey with\u00a0Airflow</h3>\n<p>Back in 2019, I started working on a project to build a customer data platform (CDP) for a client. This involved processing massive amounts of data through various microservices. Initially, we used Talend, an ETL tool, but managing it became cumbersome. Even minor changes to the pipeline or data schema required rebuilding and redeploying the entire Talend\u00a0job.</p>\n<p>This led me to explore alternatives, and that\u2019s when I came across Apache Airflow. I dug into how it worked and became convinced that it was a better solution for our needs. I presented my findings to my team, and they agreed. That\u2019s how my journey with Airflow began! Post this I worked almost for 3\u20134 years and gained much knowledged and even solved different use cases using Apache\u00a0Airflow</p>\n<p><strong>High Level Architecture Diagram</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/614/1*sTvGUcH4jjI-2rHC-vFSWA.png\"></figure><h3>About Airflow:</h3>\n<p>Apache Airflow is an open-source platform to programmatically author, schedule, and monitor workflows. Since its inception at Airbnb in 2014, Airflow has grown to become a widely adopted tool for orchestrating complex computational workflows and data processing pipelines. With its robust and extensible framework, Airflow is a top choice for data engineers and developers aiming to automate and manage their workflows.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*N2YxUJvf5046PZPv.png\"></figure><h3>Use cases</h3>\n<p>Apart form the above mentioned usecase we can solve different usecase as\u00a0below:</p>\n<ol>\n<li>\n<strong>ETL Pipelines</strong>: Airflow handles ETL(Extract, Transform, Load) tasks with grace. It schedules and monitors these pipelines, ensuring your data remains consistent and reliable, just like a diligent librarian cataloging new\u00a0books.</li>\n<li>\n<strong>Data Warehousing</strong>: Airflow helps streamline the process of loading data into warehouses, performing necessary transformations along the way making use of different operators in\u00a0built.</li>\n<li>\n<strong>Machine Learning Pipelines</strong>: Airflow can automates multiple steps like data preprocessing, model training, evaluation, and deployment ensuring your machine learning workflows run smoothly and efficiently, like a well-rehearsed symphony.</li>\n<li>\n<strong>Data Processing</strong>: Airflow coordinates tasks such as data cleaning, validation, and aggregation, acting as a reliable assistant that keeps everything on\u00a0track.</li>\n<li>\n<strong>DevOps Automation</strong>: Airflow can automate tasks like running backups, monitoring system health, and deploying applications, making it an invaluable tool for maintaining operational efficiency.</li>\n</ol>\n<h3>\n<strong>Airflow Pros </strong>\ud83d\udc4d\u00a0<strong>:</strong>\n</h3>\n<ul>\n<li>\n<strong>Scalability </strong>\ud83d\udcc8\ud83d\udd1d\ud83d\udcca: Airflow scales seamlessly, handling very large workflows and distributing tasks across multiple\u00a0workers.</li>\n<li>\n<strong>Flexibility </strong>\ud83e\udd38\u200d\u2642\ufe0f\ud83d\udd04\ud83c\udf00: With Python-based DAG definitions, You can easily customize your workflows to fit your unique requirements</li>\n<li>\n<strong>Extensibility </strong>\ud83e\udde9\ud83d\ude80\ud83d\udd27: Airflow\u2019s built-in operators(Bash Operators, Python Operators, Kubernetes Pod Operators and others) and the ability to create custom operators allow it to integrate with numerous external systems and\u00a0APIs.</li>\n<li>\n<strong>Community and Ecosystem </strong>\ud83d\udc6b\ud83c\udf0e: Being an Open source tool, Airflow benefits from a large and active community. This ensures continuous improvements and a wealth of plugins and integrations. It\u2019s like being part of a global network of experts, always ready to lend a helping\u00a0hand.</li>\n<li>\n<strong>UI and Monitoring </strong>\ud83d\udcf1\ud83d\udc41\ufe0f\u200d\ud83d\udde8\ufe0f\ud83d\udcca: The web interface provides a clear view of workflow status, logs, and task details, making debugging and monitoring a\u00a0breeze.</li>\n</ul>\n<h3>\n<strong>Airflow Cons\u00a0</strong>\ud83d\udc4e<strong>:</strong>\n</h3>\n<ul>\n<li>\n<strong>Complexity </strong>\ud83c\udf00\ud83d\udd0d\ud83d\udd27: The learning curve can be steep, especially for those new to Python or workflow orchestration. It\u2019s like learning to play a complex musical instrument\u200a\u2014\u200achallenging but rewarding once mastered.</li>\n<li>\n<strong>Resource Intensive </strong>\u2699\ufe0f\ud83d\udd0b\ud83d\udca1: Airflow can consume significant system resources, particularly for large DAGs and high-frequency task execution. Managing these resources effectively is crucial to maintain optimal performance.</li>\n<li>\n<strong>Operational Overhead </strong>\u23f3\ud83d\udd04: Maintaining an Airflow deployment, including managing the database and worker nodes, can be complex and require significant operational effort. It\u2019s like running a large-scale operation where every detail\u00a0matters.</li>\n<li>\n<strong>Dependency Management </strong>\ud83e\udde9\ud83d\udd17\ud83d\udce6: Handling Python dependencies across different tasks can be challenging, often requiring careful management of virtual environments or containers.</li>\n</ul>\n<h3>Conclusion:</h3>\n<p>Its has been almost 4\u20135 years i have been working on airflow staring from writing simple tasks and now able to deploy it in different cloud platforms liks GCP, Azure and AWS was an amazing Journey. Once you master this tool, it will be a game-changer for your data workflows.</p>\n<p>Airflow is definitely worth exploring. My experience has been overwhelmingly positive, and it\u2019s become a critical tool for our data engineering and workflow orchestration.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=2049e555f9c4\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/apache-airflow/solving-data-pipeline-challenges-with-apache-airflow-a-real-life-example-2049e555f9c4\">Solving Data Pipeline Challenges with Apache Airflow: A Real-Life Example</a> was originally published in <a href=\"https://medium.com/apache-airflow\">Apache Airflow</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["data-engineering","apache-airflow","data-pipeline-tool","airflow"]},{"title":"Airflow Installation Simplified\u200a\u2014\u200ausing Docker Compose/Podman Compose","pubDate":"2024-07-03 11:18:12","link":"https://medium.com/apache-airflow/airflow-installation-simplified-using-docker-compose-podman-compose-d840a24248ba?source=rss-3ead5bb356d4------2","guid":"https://medium.com/p/d840a24248ba","author":"Raviteja Tholupunoori","thumbnail":"","description":"\n<h3>Airflow Installation Simplified\u200a\u2014\u200ausing Docker Compose/Podman Compose</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/813/1*S5jpwblmi3AGa-iOPaiZgg.png\"></figure><p><strong>Contents</strong></p>\n<ul>\n<li>Introduction</li>\n<li>Installing Apache\u00a0Airflow</li>\n<li>Running Docker\u00a0Compose</li>\n<li>Interacting with Airflow\u00a0UI</li>\n<li>Clean up</li>\n</ul>\n<p>Github Link\u2014 <a href=\"https://github.com/raviteja10096/Airflow/tree/main/Airflow_using_Docker\">https://github.com/raviteja10096/Airflow/tree/main/Airflow_Docker</a></p>\n<h3>Introduction</h3>\n<p>Airflow is a popular tool that simplifies the complex workflow. It allows you to programmatically define, schedule, and monitor your workflows, all in one place. While Airflow is a powerful option, installation can sometimes feel overwhelming.</p>\n<p>This guide will break down the setup process into two easy-to-follow methods, getting you up and running with Airflow in no\u00a0time.</p>\n<p><strong>Sample Airflow\u00a0UI:</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*wkk42RG934LZYPnE\"></figure><h3>Installing Airflow:</h3>\n<p><strong>Airflow Components</strong></p>\n<p>We have different services like scheduler, webserver, worker, redis, postgres,flower and postgres which help you to run\u00a0airflow</p>\n<p>The <a href=\"https://github.com/raviteja10096/Airflow/blob/main/Airflow_Docker/docker-compose.yml\">docker-compose.yaml</a> file includes the following service definitions:</p>\n<ul><li>\n<strong>airflow-scheduler:</strong> Manages and schedules tasks and\u00a0DAGs.</li></ul>\n<pre>airflow-scheduler:<br>    &lt;&lt;: *airflow-common<br>    command: scheduler<br>    healthcheck:<br>      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8974/health\"]<br>      interval: 30s<br>      timeout: 10s<br>      retries: 5<br>      start_period: 30s<br>    restart: always<br>    depends_on:<br>      &lt;&lt;: *airflow-common-depends-on<br>      airflow-init:<br>        condition: service_completed_successfully</pre>\n<ul><li>\n<strong>airflow-webserver:</strong> Hosts the web interface accessible at <a href=\"http://localhost:8080/\">localhost:8080</a>\u00a0.</li></ul>\n<pre>airflow-webserver:<br>    &lt;&lt;: *airflow-common<br>    command: webserver<br>    ports:<br>      - \"8080:8080\"<br>    healthcheck:<br>      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8080/health\"]<br>      interval: 30s<br>      timeout: 10s<br>      retries: 5<br>      start_period: 30s<br>    restart: always<br>    depends_on:<br>      &lt;&lt;: *airflow-common-depends-on<br>      airflow-init:<br>        condition: service_completed_successfully</pre>\n<ul><li>\n<strong>airflow-worker: </strong>Executes tasks assigned by the scheduler.</li></ul>\n<pre>airflow-worker:<br>    &lt;&lt;: *airflow-common<br>    command: celery worker<br>    healthcheck:<br>      # yamllint disable rule:line-length<br>      test:<br>        - \"CMD-SHELL\"<br>        - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d \"celery@$${HOSTNAME}\" || celery --app airflow.executors.celery_executor.app inspect ping -d \"celery@$${HOSTNAME}\"'<br>      interval: 30s<br>      timeout: 10s<br>      retries: 5<br>      start_period: 30s<br>    environment:<br>      &lt;&lt;: *airflow-common-env<br>      # Required to handle warm shutdown of the celery workers properly<br>      # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation<br>      DUMB_INIT_SETSID: \"0\"<br>    restart: always<br>    depends_on:<br>      &lt;&lt;: *airflow-common-depends-on<br>      airflow-init:<br>        condition: service_completed_successfully</pre>\n<ul><li>\n<strong>airflow-init: </strong>Initializes the Airflow\u00a0setup.</li></ul>\n<pre>airflow-init:<br>    &lt;&lt;: *airflow-common<br>    entrypoint: /bin/bash<br>    # yamllint disable rule:line-length<br>    command:<br>      - -c<br>      - |<br>        if [[ -z \"${AIRFLOW_UID}\" ]]; then<br>          echo<br>          echo -e \"\\033[1;33mWARNING!!!: AIRFLOW_UID not set!\\e[0m\"<br>          echo \"If you are on Linux, you SHOULD follow the instructions below to set \"<br>          echo \"AIRFLOW_UID environment variable, otherwise files will be owned by root.\"<br>          echo \"For other operating systems you can get rid of the warning with manually created .env file:\"<br>          echo \"    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user\"<br>          echo<br>        fi<br>        one_meg=1048576<br>        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))<br>        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)<br>        disk_available=$$(df / | tail -1 | awk '{print $$4}')<br>        warning_resources=\"false\"<br>        if (( mem_available &lt; 4000 )) ; then<br>          echo<br>          echo -e \"\\033[1;33mWARNING!!!: Not enough memory available for Docker.\\e[0m\"<br>          echo \"At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))\"<br>          echo<br>          warning_resources=\"true\"<br>        fi<br>        if (( cpus_available &lt; 2 )); then<br>          echo<br>          echo -e \"\\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\\e[0m\"<br>          echo \"At least 2 CPUs recommended. You have $${cpus_available}\"<br>          echo<br>          warning_resources=\"true\"<br>        fi<br>        if (( disk_available &lt; one_meg * 10 )); then<br>          echo<br>          echo -e \"\\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\\e[0m\"<br>          echo \"At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))\"<br>          echo<br>          warning_resources=\"true\"<br>        fi<br>        if [[ $${warning_resources} == \"true\" ]]; then<br>          echo<br>          echo -e \"\\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\\e[0m\"<br>          echo \"Please follow the instructions to increase amount of resources available:\"<br>          echo \"   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin\"<br>          echo<br>        fi<br>        mkdir -p /sources/logs /sources/dags /sources/plugins<br>        chown -R \"${AIRFLOW_UID}:0\" /sources/{logs,dags,plugins}<br>        exec /entrypoint airflow version<br>    # yamllint enable rule:line-length<br>    environment:<br>      &lt;&lt;: *airflow-common-env<br>      _AIRFLOW_DB_MIGRATE: 'true'<br>      _AIRFLOW_WWW_USER_CREATE: 'true'<br>      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}<br>      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}<br>      _PIP_ADDITIONAL_REQUIREMENTS: ''<br>    user: \"0:0\"<br>    volumes:<br>      - ${AIRFLOW_PROJ_DIR:-.}:/sources</pre>\n<ul><li>\n<strong>flower: </strong>Monitors and provides insights into the environment. It is available at <a href=\"http://localhost:5555/\">localhost:5555</a>\u00a0.(Optional)</li></ul>\n<pre>flower:<br>    &lt;&lt;: *airflow-common<br>    command: celery flower<br>    profiles:<br>      - flower<br>    ports:<br>      - \"5555:5555\"<br>    healthcheck:<br>      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:5555/\"]<br>      interval: 30s<br>      timeout: 10s<br>      retries: 5<br>      start_period: 30s<br>    restart: always<br>    depends_on:<br>      &lt;&lt;: *airflow-common-depends-on<br>      airflow-init:<br>        condition: service_completed_successfully</pre>\n<ul><li>\n<strong>postgres:</strong> Serves as the database.</li></ul>\n<pre>postgres:<br>    image: postgres:13<br>    environment:<br>      POSTGRES_USER: airflow<br>      POSTGRES_PASSWORD: airflow<br>      POSTGRES_DB: airflow<br>    volumes:<br>      - postgres-db-volume:/var/lib/postgresql/data<br>    healthcheck:<br>      test: [\"CMD\", \"pg_isready\", \"-U\", \"airflow\"]<br>      interval: 10s<br>      retries: 5<br>      start_period: 5s<br>    restart: always</pre>\n<ul><li>\n<strong>redis: </strong>Facilitates message forwarding from the scheduler to the workers. (Optional)</li></ul>\n<pre>redis:<br>    # Redis is limited to 7.2-bookworm due to licencing change<br>    # https://redis.io/blog/redis-adopts-dual-source-available-licensing/<br>    image: redis:7.2-bookworm<br>    expose:<br>      - 6379<br>    healthcheck:<br>      test: [\"CMD\", \"redis-cli\", \"ping\"]<br>      interval: 10s<br>      timeout: 30s<br>      retries: 50<br>      start_period: 30s<br>    restart: always</pre>\n<p><strong>Airflow Volumes:</strong></p>\n<p>Besides the common environment variables for the airflow services, we have four volumes: dags, logs, config and plugins. So we need to create 4 folders in local machine for Airflow\u00a0Volumes</p>\n<ol>\n<li>dags: For placing the DAG\u00a0scripts</li>\n<li>logs: For placing logs of\u00a0airflow</li>\n<li>config: For Airflow Configurations</li>\n<li>plugins: For any Extra\u00a0Plugins</li>\n</ol>\n<p><strong>Permissions\u00a0:</strong></p>\n<p>For seamless volume synchronization, we need to confirm that the UID (user ID) and GID (group ID) permissions on the Docker volumes align with those on the local filesystem.</p>\n<p>In YAML file if you notice in line no. 74 below line which provides permissions to airflow to us the\u00a0volumes</p>\n<pre>user: \"${AIRFLOW_UID:-50000}:0</pre>\n<p>In Local Environment run below\u00a0commands</p>\n<pre>echo -e \"AIRFLOW_UID=$(id -u)\" &gt; .env<br>echo -e \"AIRFLOW_GID=0\" &gt; .env</pre>\n<p>Post this steps your\u00a0<a href=\"https://github.com/raviteja10096/Airflow/blob/main/Airflow_using_Docker/.env\">.env file</a> looks as\u00a0below</p>\n<pre>AIRFLOW_UID=501 <br>AIRFLOW_GID=0</pre>\n<p><strong>Link for full Docker Compose file</strong>\u200a\u2014\u200a<a href=\"https://github.com/raviteja10096/Airflow/blob/main/Airflow_using_Docker/docker-compose.yml\">Link</a></p>\n<h3>Run Docker\u00a0Compose:</h3>\n<p>Once we have all the setup we can run the docker compose file using below command. In my case i\u2019m using podman. You are feel free to use docker/podman.</p>\n<pre>docker-compose up airflow-init<br>or<br>podman compose up airflow-init</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*-_kxpIqCNKZIs3kt\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*nzj3ahwKSyL2T-Iu\"></figure><p>Check the services using below\u00a0command</p>\n<pre>docker compose up -d<br>or<br>podman compose up -d</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*yJ09RigUbHFio_RC\"></figure><h3>Accessing the web interface:</h3>\n<p>Once you see all the containers up and running you can open <a href=\"http://localhost:8080/\">localhost:8080</a> in browser and login to airflow with admin\u00a0creds</p>\n<p>Username\u00a0:\u00a0airflow</p>\n<p>Password\u00a0:\u00a0airflow</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*88VaGjH8WGAGvXEb\"></figure><p>Post logging in youcan see the\u00a0Dags.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*QpZ24yb_frMoMzC5\"></figure><p>We\u2019ve successfully installed the full version of Airflow in just a few minutes using\u00a0Docker.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/500/0*apE8KwtriZpNo9s3\"></figure><p>Clean up</p>\n<pre>docker compose down<br>or<br>podman compose down</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*PpiwiNn6r4w3A1LL\"></figure><p>References:</p>\n<ul>\n<li>Airflow Documentation\u200a\u2014\u200a<a href=\"https://airflow.apache.org/docs/\">https://airflow.apache.org/docs/</a>\n</li>\n<li>Git Repo Link\u200a\u2014\u200a<a href=\"https://github.com/raviteja10096/Airflow/blob/main/Airflow_using_Docker/README.md\">Link</a>\n</li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d840a24248ba\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/apache-airflow/airflow-installation-simplified-using-docker-compose-podman-compose-d840a24248ba\">Airflow Installation Simplified\u200a\u2014\u200ausing Docker Compose/Podman Compose</a> was originally published in <a href=\"https://medium.com/apache-airflow\">Apache Airflow</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<h3>Airflow Installation Simplified\u200a\u2014\u200ausing Docker Compose/Podman Compose</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/813/1*S5jpwblmi3AGa-iOPaiZgg.png\"></figure><p><strong>Contents</strong></p>\n<ul>\n<li>Introduction</li>\n<li>Installing Apache\u00a0Airflow</li>\n<li>Running Docker\u00a0Compose</li>\n<li>Interacting with Airflow\u00a0UI</li>\n<li>Clean up</li>\n</ul>\n<p>Github Link\u2014 <a href=\"https://github.com/raviteja10096/Airflow/tree/main/Airflow_using_Docker\">https://github.com/raviteja10096/Airflow/tree/main/Airflow_Docker</a></p>\n<h3>Introduction</h3>\n<p>Airflow is a popular tool that simplifies the complex workflow. It allows you to programmatically define, schedule, and monitor your workflows, all in one place. While Airflow is a powerful option, installation can sometimes feel overwhelming.</p>\n<p>This guide will break down the setup process into two easy-to-follow methods, getting you up and running with Airflow in no\u00a0time.</p>\n<p><strong>Sample Airflow\u00a0UI:</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*wkk42RG934LZYPnE\"></figure><h3>Installing Airflow:</h3>\n<p><strong>Airflow Components</strong></p>\n<p>We have different services like scheduler, webserver, worker, redis, postgres,flower and postgres which help you to run\u00a0airflow</p>\n<p>The <a href=\"https://github.com/raviteja10096/Airflow/blob/main/Airflow_Docker/docker-compose.yml\">docker-compose.yaml</a> file includes the following service definitions:</p>\n<ul><li>\n<strong>airflow-scheduler:</strong> Manages and schedules tasks and\u00a0DAGs.</li></ul>\n<pre>airflow-scheduler:<br>    &lt;&lt;: *airflow-common<br>    command: scheduler<br>    healthcheck:<br>      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8974/health\"]<br>      interval: 30s<br>      timeout: 10s<br>      retries: 5<br>      start_period: 30s<br>    restart: always<br>    depends_on:<br>      &lt;&lt;: *airflow-common-depends-on<br>      airflow-init:<br>        condition: service_completed_successfully</pre>\n<ul><li>\n<strong>airflow-webserver:</strong> Hosts the web interface accessible at <a href=\"http://localhost:8080/\">localhost:8080</a>\u00a0.</li></ul>\n<pre>airflow-webserver:<br>    &lt;&lt;: *airflow-common<br>    command: webserver<br>    ports:<br>      - \"8080:8080\"<br>    healthcheck:<br>      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8080/health\"]<br>      interval: 30s<br>      timeout: 10s<br>      retries: 5<br>      start_period: 30s<br>    restart: always<br>    depends_on:<br>      &lt;&lt;: *airflow-common-depends-on<br>      airflow-init:<br>        condition: service_completed_successfully</pre>\n<ul><li>\n<strong>airflow-worker: </strong>Executes tasks assigned by the scheduler.</li></ul>\n<pre>airflow-worker:<br>    &lt;&lt;: *airflow-common<br>    command: celery worker<br>    healthcheck:<br>      # yamllint disable rule:line-length<br>      test:<br>        - \"CMD-SHELL\"<br>        - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d \"celery@$${HOSTNAME}\" || celery --app airflow.executors.celery_executor.app inspect ping -d \"celery@$${HOSTNAME}\"'<br>      interval: 30s<br>      timeout: 10s<br>      retries: 5<br>      start_period: 30s<br>    environment:<br>      &lt;&lt;: *airflow-common-env<br>      # Required to handle warm shutdown of the celery workers properly<br>      # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation<br>      DUMB_INIT_SETSID: \"0\"<br>    restart: always<br>    depends_on:<br>      &lt;&lt;: *airflow-common-depends-on<br>      airflow-init:<br>        condition: service_completed_successfully</pre>\n<ul><li>\n<strong>airflow-init: </strong>Initializes the Airflow\u00a0setup.</li></ul>\n<pre>airflow-init:<br>    &lt;&lt;: *airflow-common<br>    entrypoint: /bin/bash<br>    # yamllint disable rule:line-length<br>    command:<br>      - -c<br>      - |<br>        if [[ -z \"${AIRFLOW_UID}\" ]]; then<br>          echo<br>          echo -e \"\\033[1;33mWARNING!!!: AIRFLOW_UID not set!\\e[0m\"<br>          echo \"If you are on Linux, you SHOULD follow the instructions below to set \"<br>          echo \"AIRFLOW_UID environment variable, otherwise files will be owned by root.\"<br>          echo \"For other operating systems you can get rid of the warning with manually created .env file:\"<br>          echo \"    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user\"<br>          echo<br>        fi<br>        one_meg=1048576<br>        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))<br>        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)<br>        disk_available=$$(df / | tail -1 | awk '{print $$4}')<br>        warning_resources=\"false\"<br>        if (( mem_available &lt; 4000 )) ; then<br>          echo<br>          echo -e \"\\033[1;33mWARNING!!!: Not enough memory available for Docker.\\e[0m\"<br>          echo \"At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))\"<br>          echo<br>          warning_resources=\"true\"<br>        fi<br>        if (( cpus_available &lt; 2 )); then<br>          echo<br>          echo -e \"\\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\\e[0m\"<br>          echo \"At least 2 CPUs recommended. You have $${cpus_available}\"<br>          echo<br>          warning_resources=\"true\"<br>        fi<br>        if (( disk_available &lt; one_meg * 10 )); then<br>          echo<br>          echo -e \"\\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\\e[0m\"<br>          echo \"At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))\"<br>          echo<br>          warning_resources=\"true\"<br>        fi<br>        if [[ $${warning_resources} == \"true\" ]]; then<br>          echo<br>          echo -e \"\\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\\e[0m\"<br>          echo \"Please follow the instructions to increase amount of resources available:\"<br>          echo \"   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin\"<br>          echo<br>        fi<br>        mkdir -p /sources/logs /sources/dags /sources/plugins<br>        chown -R \"${AIRFLOW_UID}:0\" /sources/{logs,dags,plugins}<br>        exec /entrypoint airflow version<br>    # yamllint enable rule:line-length<br>    environment:<br>      &lt;&lt;: *airflow-common-env<br>      _AIRFLOW_DB_MIGRATE: 'true'<br>      _AIRFLOW_WWW_USER_CREATE: 'true'<br>      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}<br>      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}<br>      _PIP_ADDITIONAL_REQUIREMENTS: ''<br>    user: \"0:0\"<br>    volumes:<br>      - ${AIRFLOW_PROJ_DIR:-.}:/sources</pre>\n<ul><li>\n<strong>flower: </strong>Monitors and provides insights into the environment. It is available at <a href=\"http://localhost:5555/\">localhost:5555</a>\u00a0.(Optional)</li></ul>\n<pre>flower:<br>    &lt;&lt;: *airflow-common<br>    command: celery flower<br>    profiles:<br>      - flower<br>    ports:<br>      - \"5555:5555\"<br>    healthcheck:<br>      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:5555/\"]<br>      interval: 30s<br>      timeout: 10s<br>      retries: 5<br>      start_period: 30s<br>    restart: always<br>    depends_on:<br>      &lt;&lt;: *airflow-common-depends-on<br>      airflow-init:<br>        condition: service_completed_successfully</pre>\n<ul><li>\n<strong>postgres:</strong> Serves as the database.</li></ul>\n<pre>postgres:<br>    image: postgres:13<br>    environment:<br>      POSTGRES_USER: airflow<br>      POSTGRES_PASSWORD: airflow<br>      POSTGRES_DB: airflow<br>    volumes:<br>      - postgres-db-volume:/var/lib/postgresql/data<br>    healthcheck:<br>      test: [\"CMD\", \"pg_isready\", \"-U\", \"airflow\"]<br>      interval: 10s<br>      retries: 5<br>      start_period: 5s<br>    restart: always</pre>\n<ul><li>\n<strong>redis: </strong>Facilitates message forwarding from the scheduler to the workers. (Optional)</li></ul>\n<pre>redis:<br>    # Redis is limited to 7.2-bookworm due to licencing change<br>    # https://redis.io/blog/redis-adopts-dual-source-available-licensing/<br>    image: redis:7.2-bookworm<br>    expose:<br>      - 6379<br>    healthcheck:<br>      test: [\"CMD\", \"redis-cli\", \"ping\"]<br>      interval: 10s<br>      timeout: 30s<br>      retries: 50<br>      start_period: 30s<br>    restart: always</pre>\n<p><strong>Airflow Volumes:</strong></p>\n<p>Besides the common environment variables for the airflow services, we have four volumes: dags, logs, config and plugins. So we need to create 4 folders in local machine for Airflow\u00a0Volumes</p>\n<ol>\n<li>dags: For placing the DAG\u00a0scripts</li>\n<li>logs: For placing logs of\u00a0airflow</li>\n<li>config: For Airflow Configurations</li>\n<li>plugins: For any Extra\u00a0Plugins</li>\n</ol>\n<p><strong>Permissions\u00a0:</strong></p>\n<p>For seamless volume synchronization, we need to confirm that the UID (user ID) and GID (group ID) permissions on the Docker volumes align with those on the local filesystem.</p>\n<p>In YAML file if you notice in line no. 74 below line which provides permissions to airflow to us the\u00a0volumes</p>\n<pre>user: \"${AIRFLOW_UID:-50000}:0</pre>\n<p>In Local Environment run below\u00a0commands</p>\n<pre>echo -e \"AIRFLOW_UID=$(id -u)\" &gt; .env<br>echo -e \"AIRFLOW_GID=0\" &gt; .env</pre>\n<p>Post this steps your\u00a0<a href=\"https://github.com/raviteja10096/Airflow/blob/main/Airflow_using_Docker/.env\">.env file</a> looks as\u00a0below</p>\n<pre>AIRFLOW_UID=501 <br>AIRFLOW_GID=0</pre>\n<p><strong>Link for full Docker Compose file</strong>\u200a\u2014\u200a<a href=\"https://github.com/raviteja10096/Airflow/blob/main/Airflow_using_Docker/docker-compose.yml\">Link</a></p>\n<h3>Run Docker\u00a0Compose:</h3>\n<p>Once we have all the setup we can run the docker compose file using below command. In my case i\u2019m using podman. You are feel free to use docker/podman.</p>\n<pre>docker-compose up airflow-init<br>or<br>podman compose up airflow-init</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*-_kxpIqCNKZIs3kt\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*nzj3ahwKSyL2T-Iu\"></figure><p>Check the services using below\u00a0command</p>\n<pre>docker compose up -d<br>or<br>podman compose up -d</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*yJ09RigUbHFio_RC\"></figure><h3>Accessing the web interface:</h3>\n<p>Once you see all the containers up and running you can open <a href=\"http://localhost:8080/\">localhost:8080</a> in browser and login to airflow with admin\u00a0creds</p>\n<p>Username\u00a0:\u00a0airflow</p>\n<p>Password\u00a0:\u00a0airflow</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*88VaGjH8WGAGvXEb\"></figure><p>Post logging in youcan see the\u00a0Dags.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*QpZ24yb_frMoMzC5\"></figure><p>We\u2019ve successfully installed the full version of Airflow in just a few minutes using\u00a0Docker.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/500/0*apE8KwtriZpNo9s3\"></figure><p>Clean up</p>\n<pre>docker compose down<br>or<br>podman compose down</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*PpiwiNn6r4w3A1LL\"></figure><p>References:</p>\n<ul>\n<li>Airflow Documentation\u200a\u2014\u200a<a href=\"https://airflow.apache.org/docs/\">https://airflow.apache.org/docs/</a>\n</li>\n<li>Git Repo Link\u200a\u2014\u200a<a href=\"https://github.com/raviteja10096/Airflow/blob/main/Airflow_using_Docker/README.md\">Link</a>\n</li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d840a24248ba\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/apache-airflow/airflow-installation-simplified-using-docker-compose-podman-compose-d840a24248ba\">Airflow Installation Simplified\u200a\u2014\u200ausing Docker Compose/Podman Compose</a> was originally published in <a href=\"https://medium.com/apache-airflow\">Apache Airflow</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["apache-airflow","airflow-installation","airflow"]}]}